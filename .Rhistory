# Annahmen:(X,Y) sei bivariat stetig verteilt und (X_1,Y_1),...,(X_n,Y_n) seien unabh?ngig
# und identisch verteilt wie (X,Y).
# Zu testen:
#  H_0 : X und Y sind unabh?ngig (??? ?? = 0)
# gegen
# H1 : ?? /=  0 (??? X und Y sind monoton assoziiert, also abh?ngig)
# Befehl:
cor.test(daten1,daten2, alternative = c("two.sided", "less", "greater"), method = c("kendall"),
exact = NULL, conf.level = 0.95, continuity = FALSE))
# Bemerkung:
#  Beziehung zwischen ??S und ??:
#- ??S ist zwar meist numerisch extremer als ??, aber die Varianz von ??S ist kleiner als die von ?? weswegen
# auf Basis von ?? eine Abweichung von der Unabh?ngigkeitshypothese tendenziell eher als signi???kant erkl?rt wird.
# Interpretation:
#  ??  misst  den Grad des monotonen Zusammenhangs zwischen den Realisierungen von X und Y:
#  Je n?her der Koeffizient bei +1 oder ???1 ist, umso enger gruppieren sich die Datenpunkte (xi,yi) in einem
# monotonen Verlauf.
# Allerdings keine Aussage wie steil der monotone Trend ist.
#________|Allgemeine Abh?ngigkeit|_________#####
# Der Hoe???dings Test ##############
#Hoe???dings Test ist in der Lage, auch - gewisse - nicht-monotone Abh?ngigkeiten zu entdecken,
# die den anderen drei Tests verborgen bleiben. Er ist in der Funktion hoeffd() des Paketes Hmisc implementiert.
"install.packages(""Hmisc""),
library(""Hmisc""),
hoeffd()"
#************************************************************************************************####
#*********Test auf G?ltigkeit der Annahmen*************#####
#************************************************************************************************####
#___|multivariate Normalverteiltheit pr?fen|___________#####
#multivariate Normalverteilt####
# R-Paket MVN:
# Befehl:
mvn(data, subset = NULL, mvnTest = c("mardia", "hz", "royston", "dh",
"energy"), covariance = TRUE, tol = 1e-25, alpha = 0.5,
scale = FALSE, desc = TRUE, transform = "none", R = 1000,
univariateTest = c("SW", "CVM", "Lillie", "SF", "AD"),
univariatePlot = "none", multivariatePlot = "none",
multivariateOutlierMethod = "none", bc = FALSE, bcType = "rounded",
showOutliers = FALSE, showNewData = FALSE)
#__________|Unabh?ngigkeit pr?fen|___________#####
# Streudiagramm: Erkennen von Trends, bzw Assoziation
plot(data_1, data_2)
identify(data_1, data_2, n = 3)
n = #Anzahl der Beobachtungen Nr.1,Nr.2,..., hier bis Nr.3
# Beobachtung: Ist ein Trend zu erkennen? Wom?glich ein paar ausreiser dabei.
# Zeitreihen: Erkennen von Trends ((monotones)-Wachstum) oder Saisonalit?ten (Peridozit?t).
ts.plot(data_1, xlab = "Zeit",  ylab = "data_1")
ts.plot(data_2, + xlab = "Zeit",  ylab = "data_2")
# ==> Trends oder Saisonalit?ten ==> m?gliche Abh?ngigkeit.
# Autokorrelation: Erkennen von korrelationen (?bersteigen der blauen linie au?er von 0))
acf(data_1)
acf(data_2)
# ==> keine Autokorrelation von X_i und Y_i sprechen nicht gegen eine Annahme von Unabh?ngigkeit,
# also Autokorreliert => Eventuell Abh?ngig.
#************************************************************************************************####
#*****************Lineare Regression*****************#####
#************************************************************************************************####
#Lineare Regression######
# In der einfachen linearen Regression
# Y_i = ??_0 + ??_1x_i + ??_i f?r i = 1,...,n
# sollen Sch?tzer ??_0 und ??_1 f?r ??_0 bzw. ??_1 bestimmt werden, die den (z. B. euklidischen) Abstand
# zwischen dem Beobachtungsvektor (Y_1,...,Y_n) und dem Vektor der Regressionsfunktionswerte
# (b_0 +b_1x_1,...,b_0 +b_1x_n) in (b_0,b_1) minimieren (mit Hilfe der sog. Kleinste-Quadrate-Methode).
# Befehl:
lm(formula=data.df[i]~data.df[j],data.df)
# Die Tilde ~ im Argument formula bedeutet, dass die links von ihr stehende Variable von der
# rechts von ihr stehenden " abh?ngen" soll.
# Bsp:
#  Data Frame airquality,
Oz <- lm(formula = Ozone ~ Temp, data = airquality)
# Das Ausgeben der Koeffizienten f?r die Regressionsgerade
Call: lm(formula = Ozone ~ Temp, data = airquality)
# Das Plotten von Streudiagramm und der Regressionsgeraden
with(airquality, plot(Temp, Ozone))
abline(Oz)
#***********************************************************************************************####
#  Testg?te Lokationsproblem (Normalverteilt) *****************#####
#************************************************************************************************####
# G?te#####
# G?te := 1????? ??? P_{H_1} (H0 wird verworfen)
#Der zweiseitige Einstichproben-Gau?test #####
# Annahmen: X_1,...,X_n u.i.v. ??? N(?,??2) mit unbekanntem ? und bekanntem (!) ??^2.
# Zu testen: H_0 : ? = ?_0 gegen H1 : ? /= ?_0 zum Signi???kanzniveau ??.
# Befehl:
power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05,
power = NULL,
type = c("two.sample", "one.sample", "paired"),
alternative = c("two.sided", "one.sided"),
strict = FALSE, tol = .Machine$double.eps^0.25)
# Bemerkung:
#   1. Bestimme zu gegebenen |???| und n die damit erreichbare G?te 1?????!
#      power.t.test(n = 50, delta = 0.3, sd = 0.9, type = "one.sample")
#      => power=G?te = 1?????
#   2. Berechne zu gegebenen |???| und 1????? den dazu n?otigen Stichprobenumfang n!
#      power.t.test(delta = 0.3, power = 0.8, sd = 0.9, type = "one.sample")
#      ==>n = 72.12412=73
#   3 Ermittle zu gegebenen n und 1????? den kleinsten entdeckbaren Unterschied |???|!
#      power.t.test(n = 50, power = 0.8, sd = 0.9, type = "one.sample")
#      ==> delta = 0.3637661
#   4  Man kann auch zu festem n und verschiedenen |???| die G?ten bestimmen:
#      power.t.test(n = 50, delta = c(0, 0.1, 0.2, 0.3, 0.4), sd = 0.9,  type = "one.sample")
# Bemerkung:
# 1.Die G?te hat sich also zu einer Funktion von ??? := ?????_0 , ??, n und ?? konkretisiert:
# G?te = 1?????(???,??,n,??)  ?????? ??, falls ???n|???|/?? ?????? 0
#                      ?????? 1, "          "  ?????? ???
# Eine genauere Inspektion dieser G?tefunktion zeigt, dass sie symmetrisch in ??? ist
# und bei Fixierung der jeweils ?brigen Argumente ...
# ... in |???| streng monoton w?chst,
#     "   ??    "      " f?llt,
#     "   n    "      " w?chst und
#     "   ??    "      " w?chst.
# 2. Wenn einseitiger Test gen?gt sollte man dies machen,da es sich g?nstig auf Stichprobenumfang
# und/oder G?te auswirkt. (Also der einseitige Einstichproben-t-Test)
# Der einseitige Einstichproben-t-Test #####
# Annahmen: X_1,...,X_n u.i.v. ??? N(?,??^2) mit unbekannten ? und ??^2.
# Zu testen:
#            H'_0 : ? ??? ?_0 gegen H'_1 : ? < ?_0 zum Signi???kanzniveau ??
# bzw.
#            H'_0 : ? ??? ?_0 gegen H'_1 : ? > ?_0 zum Signi???kanzniveau ??
# Bemerkung:
#   1. Bestimme zu gegebenen |???| und n die damit erreichbare G?te 1?????!
power.t.test(n = 50, delta = 0.3, sd = 0.9, alternative = "one.sided", type = "one.sample")
#     => power=G?te = 1?????
#   2. Berechne zu gegebenen |???| und 1????? den dazu n?tigen Stichprobenumfang n!
power.t.test(delta = 0.3, power = 0.8, sd = 0.9, type = "one.sample", type = "one.sample")
#       ==>n =
#   3. Ermittle zu gegebenen n und 1????? den kleinsten entdeckbaren Unterschied |???|!
power.t.test(n = 50, power = 0.8, sd = 0.9, type = "one.sample", type = "one.sample")
#     ==> delta =
# Zwei unverbundene Stichproben  #####
# Annahmen: X_1,X_2,... u. i. v. ??? N(?_X,??^2_X) mit unbekannten ?_X und ??^2_X, sowie unabh?ngig davon
#          y_1,y_2,... u. i. v. ??? N(?_y,??^2_y) mit unbekannten ?_y und ??^2_y
# Zu testen: H_0 : ?_X = ?_Y gegen H_1 : ?_X /= ?_Y zum Signi???kanzniveau ?? und ??_X = ??_Y = ??= 0.9.
# Bemerkung:
#  1. Bestimme zu gegebenen |???| und n die damit erreichbare G?te 1?????!
power.t.test(n = 50, delta = 0.3, sd = 0.9,type = "two.sample")
#      ==> power=G?te = 1?????
#  2. Berechne zu gegebenen |???| und 1????? den dazu n?tigen Stichprobenumfang n!
power.t.test(delta = 0.3, power = 0.8, sd = 0.9,  type = "two.sample")
#      ==>n =
#  3. Ermittle zu gegebenen n und 1????? den kleinsten entdeckbaren Unterschied |???|!
power.t.test(n = 50, power = 0.8, sd = 0.9, type = "two.sample")
#      ==> delta =
# Zwei verbundene Stichproben  #####
#   Annahmen: D_i := X_i ???Y_i u. i. v. ??? N(?_D,??^2_D) mit unbekannten ?_D und ??^2_D.
#   Zu testen: H0 : ?D = ?0 gegen H1 : ?D 6= ?0 bzw. einseitg zum Signi???kanzniveau ??.
#   Bemerkung:
#     1. Bestimme zu gegebenen |???| und n die damit erreichbare G?te 1?????!
power.t.test(n = 50, delta = 0.3, sd = 0.9, type = "paired")
#   ==> power=G?te = 1?????
#   2. Berechne zu gegebenen |???| und 1????? den dazu n?tigen Stichprobenumfang n!
power.t.test(delta = 0.3, power = 0.8, sd = 0.9,  type = "paired")
#   ==>n =
#   3. Ermittle zu gegebenen n und 1????? den kleinsten entdeckbaren Unterschied |???|!
power.t.test(n = 50, power = 0.8, sd = 0.9, type = "paired")
#   ==> delta =
#************************************************************************************************####
install.packages("readxl")
library(readxl)
version
my_data <- read.table(water_quality.xls)
my_data <- read.table(water_quality.xlsx)
library(readxl)
water_quality <- read_excel("water_quality.xlsx")
View(water_quality)
1918*8
1918*8-13699
install.packages("tidyverse")
x <- randu(1)
dnorm()
dnorm(0)
dnorm(x)
A <- [1,2,3;4,5,6]
A <- (1,2,3;4,5,6)
A <- c(1,2,3;4,5,6)
A <- matrix(seq(1,2,0,1,0,-6,1,1,0,0,1,-4,3,0,0,0,1,-6,0,0,-1,2,0,1,0,0,1,-1,0,-1,0,0,1,0,1,-4,-1,0,0,0,0,0,0,-1,0,0,0,0,0,0,-1,0,0,0,0,0,0,-1,0,0,0,0,0,0,-1,0), nrow = 11, ncol = 6)
set.seed(1)
x <- rnorm(10, mean = 1, sd = 4)
lm(y ~ 1)
lm(x ~ 1)
mean(x)
var(x)
x_1 <- rnorm(10, mean = 1, sd = 4)
x_2<-rnoem
x_2<-rnorm(12, mean = 2, sd = 1 )
help(lm)
lm(y~x_1+x_2)
help(ggplot)
help("ggplot")
areaA1 <- c(59,65,70,66)
areaA2 <- c(67,69,72,70)
t.test(areaA1,areaA2)
persp(x, y, z)
x <- -20:20
y <- 0:20
z_val <- function(x, y) {  sqrt(x  - sqrt(y)}
z_val <- function(x, y) {  sqrt(x  - sqrt(y))}
z<-outer(x, y, z_val)
x <- 0:20
y <- 0:20
x <- -20:20
y <- 0: x^2
z<-outer(x, y, z_val)
z_val <- function(x, y) {  sqrt(x  - sqrt(y))}
z<-outer(x, y, z_val)
persp(x, y, z)
#  function to view the first k lines of a data frame
view <- function(dat,k){
message <- paste("First",k,"rows")
krows <- dat[1:k,]
cat(message,"\n","\n")
print(krows)
}
#  function to calculate summary statistics across the 1000
#  data sets
simsum <- function(dat,trueval){
S <- nrow(dat)
MCmean <- apply(dat,2,mean)
MCbias <- MCmean-trueval
MCrelbias <- MCbias/trueval
MCstddev <- sqrt(apply(dat,2,var))
MCMSE <- apply((dat-trueval)^2,2,mean)
#   MCMSE <- MCbias^2 + MCstddev^2   # alternative lazy calculation
MCRE <- MCMSE[1]/MCMSE
sumdat <- rbind(rep(trueval,3),S,MCmean,MCbias,MCrelbias,MCstddev,MCMSE,
MCRE)
names <- c("true value","# sims","MC mean","MC bias","MC relative bias",
"MC standard deviation","MC MSE","MC relative efficiency")
ests <- c("Sample mean","Trimmed mean","Median")
dimnames(sumdat) <- list(names,ests)
round(sumdat,5)
}
#  function to generate S data sets of size n from normal
#  distribution with mean mu and variance sigma^2
generate.normal <- function(S,n,mu,sigma){
dat <- matrix(rnorm(n*S,mu,sigma),ncol=n,byrow=T)
#  Note: for this very simple data generation, we can get the data
#  in one step like this, which requires no looping.  In more complex
#  statistical models, looping is often required to set up each
#  data set, because the scenario is much more complicated.  Here is
#  a loop to get the same data as above; try running the program and see
#  how much longer it takes!
#  dat <- NULL
#
#   for (i in 1:S){
#
#      Y <- rnorm(n,mu,sigma)
#      dat <- rbind(dat,Y)
#
#   }
out <- list(dat=dat)
return(out)
}
#  function to generate S data sets of size n from gamma
#  distribution with mean mu, variance sigma^2 mu^2
generate.gamma <- function(S,n,mu,sigma){
a <- 1/(sigma^2)
s <- mu/a
dat <- matrix(rgamma(n*S,shape=a,scale=s),ncol=n,byrow=T)
#  Alternative loop
#   dat <- NULL
#
#   for (i in 1:S){
#
#      Y <- rgamma(n,shape=a,scale=s)
#      dat <- rbind(dat,Y)
#
#   }
out <- list(dat=dat)
return(out)
}
#  function to generate S data sets of size n from a t distribution
#  with df degrees of freedom centered at the value mu (a t distribution
#  has mean 0 and variance df/(df-2) for df>2)
generate.t <- function(S,n,mu,df){
dat <- matrix(mu + rt(n*S,df),ncol=n,byrow=T)
#   Alternative loop
#   dat <- NULL
#
#   for (i in 1:S){
#
#      Y <- mu + rt(n,df)
#      dat <- rbind(dat,Y)
#
#   }
out <- list(dat=dat)
return(out)
}
#  function to compute the 20% trimmed mean
trimmean <- function(Y){mean(Y,0.2)}
install.packages("languageserver")
system2(command = "echo", args = "$PATH")
install.packages("languageserver")
install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR(admin_password = 'Admin user password')
IRkernel::installspec(user = FALSE)
par(mfrow = 1:2, las = 1, font = 2, font.axis = 2, font.lab = 4,
mar = c(5, 4, 1, 2), bty = "n", xaxs = "i", yaxs = "i")
edfG1(seq(100, 350, 10))
install.packages("bbmle")
axis(2, seq(0.1, 0.9, 0.1))
## A couple of survival functions
## ==============================
mu <- 3; si <- 2
al <- 0.5; la <- 55
windows(width = 15)
par(mfrow = 1:2, las = 1, font = 2, font.axis = 2, font.lab = 4,
mar = c(5, 4, 1, 2), bty = "n", xaxs = "i", yaxs = "i")
curve(plnorm(x, mu, si, lower.tail = FALSE), from = 0, to = 100, lwd = 3,
xlab = "Time", ylab = "S(t)", ylim = 0:1)
axis(1, seq(10, 90, 20))
axis(2, seq(0.1, 0.9, 0.1))
curve(pweibull(x, al, la, lower.tail = FALSE), from = 0, to = 100, lwd = 3,
xlab = "Time", ylab = "S(t)", ylim = 0:1)
axis(1, seq(10, 90, 20))
axis(2, seq(0.1, 0.9, 0.1))
## What shape do the corresponding hazard functions have?
## ------------------------------------------------------
library(eha)
install.packages("eha")
## What shape do the corresponding hazard functions have?
## ------------------------------------------------------
library(eha)
windows(width = 15)
par(mfrow = 1:2, las = 1, font = 2, font.axis = 2, font.lab = 4,
mar = c(5, 4, 1, 2), bty = "n", xaxs = "i", yaxs = "i")
curve(hlnorm(x, mu, si), from = 0, to = 100, lwd = 3, ylim = c(0, 0.07),
xlab = "Time", ylab = expression(lambda(t)))
axis(1, seq(10, 90, 20))
curve(hweibull(x, al, la), from = 0, to = 100, lwd = 3, ylim = c(0, 0.07),
xlab = "Time", ylab = expression(lambda(t)))
axis(1, seq(10, 90, 20))
windows(width = 15)
par(mfrow = 1:2, las = 1, font = 2, font.axis = 2, font.lab = 4,
mar = c(5, 4, 1, 2), bty = "n", xaxs = "i", yaxs = "i")
curve(hlnorm(x, mu, si), from = 0, to = 10, lwd = 3, ylim = c(0, 0.08),
xlab = "Time", ylab = expression(lambda(t)))
axis(1, seq(1, 9, 2))
curve(hweibull(x, al, la), from = 0, to = 10, lwd = 3, ylim = c(0, 0.08),
xlab = "Time", ylab = expression(lambda(t)))
axis(1, seq(1, 9, 2))
## =================================
## "Illustration DMBA" (Slide 48/94)
## =================================
ratcancer <- data.frame(group = factor(rep(c("Group 1", "Group 2"), c(19, 21))),
times = c(143, 164, 188, 188, 190, 192, 206, 209, 213,
216, 220, 227, 230, 234, 246, 265, 304, 216,
244, 142, 156, 163, 198, 205, 232, 232, 233,
233, 233, 233, 239, 240, 261, 280, 280, 296,
296, 323, 204, 344),
cens = rep(c(1, 0, 1, 0), c(17, 2, 19, 2)))
summary(ratcancer)
## Empirical distribution function (ignoring right censoring)
## ----------------------------------------------------------
edfG1 <- with(subset(ratcancer, group == "Group 1"), ecdf(times))
## Empirical distribution function (ignoring right censoring)
## ----------------------------------------------------------
edfG1 <- with(subset(ratcancer, group == "Group 1"), ecdf(times))
edfG1
edfG1(seq(100, 350, 10))
maxtim <- max(ratcancer$times)
(srvedfG1 <- round(1 - edfG1(0:maxtim), 3))
edfG2 <- with(subset(ratcancer, group == "Group 2"), ecdf(times))
srvedfG2 <- round(1 - edfG2(0:maxtim), 3)
install.packages("fitdistrplus")
5/60.5
## install required packages
#install.packages("pcalg")
#install.packages("bnlearn")
#devtools::install_github("bips-hb/tpc")
#devtools::install_github("bips-hb/micd")
## load required packages
library(bnlearn)
library(pcalg)
library(micd)
library(tpc)
## load cohort data and create cross-sectional version of the dataset
orders <- read.csv("train.csv", header=TRUE, sep=',')
setwd("~/Downloads/TUM/4.WS2022(Erasmus)/datathonfme2022/accenture_challenge")
## install required packages
#install.packages("pcalg")
#install.packages("bnlearn")
#devtools::install_github("bips-hb/tpc")
#devtools::install_github("bips-hb/micd")
## load required packages
library(bnlearn)
library(pcalg)
library(micd)
library(tpc)
library(tidyverse)
## load cohort data and create cross-sectional version of the dataset
orders <- read.csv("train.csv", header=TRUE, sep=',')
dat_cross <- orders[ ,1:11]
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = disCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
## install required packages
#install.packages("pcalg")
#install.packages("bnlearn")
#devtools::install_github("bips-hb/tpc")
#devtools::install_github("bips-hb/micd")
## load required packages
library(bnlearn)
library(pcalg)
library(micd)
library(tpc)
library(tidyverse)
## load cohort data and create cross-sectional version of the dataset
orders <- read.csv("train.csv", header=TRUE, sep=',')
dat_cross <- orders[ ,1:11]
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = disCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = gaussCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
graphviz.plot(g, shape = "ellipse")
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
bnlearn_fit_mix <- pc.stable(dat_cross, test = "mi-cg", alpha = 0.01)
graphviz.plot(bnlearn_fit_mix, shape = "ellipse")
## install required packages
#install.packages("pcalg")
#install.packages("bnlearn")
#devtools::install_github("bips-hb/tpc")
#devtools::install_github("bips-hb/micd")
## load required packages
library(bnlearn)
library(pcalg)
library(micd)
library(tpc)
library(tidyverse)
## load cohort data and create cross-sectional version of the dataset
orders <- read.csv("train.csv", header=TRUE, sep=',')
dat_cross <- orders[ ,1:11]
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.005,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.05,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl = TRUE)
mygraph(pcalg_fit_mix)
help(pc)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.01,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable.fast", maj.rule = TRUE, solve.confl =TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.05,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable.fast", maj.rule = TRUE, solve.confl =TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.005,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable.fast", maj.rule = TRUE, solve.confl =TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.005,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl =TRUE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
pcalg_fit_mix <- pc(suffStat = dat_cross, indepTest = mixCItest, alpha = 0.05,
labels = colnames(dat_cross), u2pd="relaxed",
skel.method = "stable", maj.rule = TRUE, solve.confl =FALSE)
mygraph <- function(pcgraph){
g <- as.bn(pcgraph, check.cycles = FALSE)
graphviz.plot(g, shape = "ellipse")
}
mygraph(pcalg_fit_mix)
